# Contributing to WronAI ğŸ¤

DziÄ™kujemy za zainteresowanie wspÃ³Å‚pracÄ… z projektem WronAI! TwÃ³j wkÅ‚ad pomoÅ¼e w rozwoju polskiej sztucznej inteligencji.

## ğŸ¯ Jak moÅ¼esz pomÃ³c

### ğŸ› ZgÅ‚aszanie bÅ‚Ä™dÃ³w
- SprawdÅº czy bÅ‚Ä…d nie zostaÅ‚ juÅ¼ zgÅ‚oszony w [Issues](https://github.com/twoje-repo/WronAI/issues)
- UÅ¼yj template dla bug reportÃ³w
- DoÅ‚Ä…cz informacje o systemie i reprodukcjÄ™ bÅ‚Ä™du

### ğŸ’¡ Propozycje funkcji
- OtwÃ³rz [Feature Request](https://github.com/twoje-repo/WronAI/issues/new)
- Opisz przypadek uÅ¼ycia i korzyÅ›ci
- Zaproponuj implementacjÄ™ jeÅ›li moÅ¼liwe

### ğŸ“ Poprawa dokumentacji
- Poprawki literÃ³wek i bÅ‚Ä™dÃ³w
- Dodawanie przykÅ‚adÃ³w uÅ¼ycia
- TÅ‚umaczenia dokumentacji

### ğŸ”§ Kod i implementacja
- Nowe features zgodne z roadmap
- Optymalizacje wydajnoÅ›ci
- Dodawanie testÃ³w

## ğŸš€ Proces wspÃ³Å‚pracy

### 1. Setup Å›rodowiska

```bash
# Fork repozytorium na GitHub
git clone https://github.com/twoje-username/WronAI.git
cd WronAI

# StwÃ³rz virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Zainstaluj dependencies
pip install -e ".[dev]"

# Zainstaluj pre-commit hooks
pre-commit install
```

### 2. Tworzenie branch

```bash
# StwÃ³rz nowy branch dla swojej funkcji
git checkout -b feature/nazwa-funkcji

# Lub dla bugfix
git checkout -b bugfix/opis-bledu

# Lub dla dokumentacji
git checkout -b docs/opis-zmiany
```

### 3. Development

#### Code Style
- UÅ¼ywamy **Black** dla formatowania kodu
- **Flake8** dla linting
- **mypy** dla type checking
- Maksymalna dÅ‚ugoÅ›Ä‡ linii: 88 znakÃ³w

```bash
# Format kodu
black .

# SprawdÅº style
flake8 .

# Type checking
mypy scripts/ models/
```

#### Testy
- Pisz testy dla nowych funkcji
- Uruchamiaj testy przed commitem

```bash
# Uruchom wszystkie testy
pytest

# Testy z coverage
pytest --cov=wronai --cov-report=html
```

#### Commitowanie
- UÅ¼ywaj [Conventional Commits](https://www.conventionalcommits.org/)
- Podnaj zmiany w maÅ‚ych, logicznych commitach

```bash
# PrzykÅ‚ady commitÃ³w
git commit -m "feat: add Polish BERT tokenizer support"
git commit -m "fix: resolve memory leak in training loop"
git commit -m "docs: update installation instructions"
git commit -m "test: add unit tests for data preprocessing"
git commit -m "perf: optimize model inference speed"
git commit -m "refactor: simplify configuration loading"
```

### 4. Pull Request

#### Przed wysÅ‚aniem PR
- [ ] Kod jest sformatowany (black, flake8)
- [ ] Testy przechodzÄ…
- [ ] Dokumentacja jest zaktualizowana
- [ ] CHANGELOG.md jest zaktualizowany
- [ ] Commit messages sÄ… prawidÅ‚owe

#### Template PR
```markdown
## Opis zmian
KrÃ³tki opis tego co zostaÅ‚o zmienione i dlaczego.

## Typ zmiany
- [ ] Bug fix (non-breaking change)
- [ ] New feature (non-breaking change)
- [ ] Breaking change
- [ ] Documentation update

## Testy
- [ ] Dodano nowe testy
- [ ] Wszystkie testy przechodzÄ…
- [ ] Manual testing przeprowadzony

## Checklist
- [ ] Self-review kodu
- [ ] Komentarze w trudnych fragmentach
- [ ] Dokumentacja zaktualizowana
- [ ] Brak conflictÃ³w z main branch
```

## ğŸ“‹ Standardy kodu

### Python Code Style
```python
# Dobre praktyki
def process_polish_text(text: str, normalize: bool = True) -> str:
    """
    Process Polish text for model training.
    
    Args:
        text: Input Polish text
        normalize: Whether to normalize whitespace
        
    Returns:
        Processed text ready for tokenization
    """
    if not text or not isinstance(text, str):
        raise ValueError("Text must be a non-empty string")
    
    # Implementation details...
    return processed_text

# Type hints wszÄ™dzie gdzie moÅ¼liwe
from typing import List, Dict, Optional, Union

def train_model(
    config: Dict[str, Any],
    dataset: Optional[Dataset] = None
) -> Tuple[AutoModel, float]:
    """Train WronAI model with given configuration."""
    pass
```

### Dokumentacja docstring
```python
def fine_tune_model(
    model_name: str,
    dataset_path: str,
    output_dir: str,
    learning_rate: float = 2e-4,
    epochs: int = 3
) -> str:
    """
    Fine-tune Polish language model using QLoRA.
    
    This function implements efficient fine-tuning for Polish LLMs
    using 4-bit quantization and LoRA adapters to minimize
    memory requirements while maintaining model quality.
    
    Args:
        model_name: Name of base model (e.g., 'mistralai/Mistral-7B-v0.1')
        dataset_path: Path to Polish instruction dataset
        output_dir: Directory to save fine-tuned model
        learning_rate: Learning rate for training
        epochs: Number of training epochs
        
    Returns:
        Path to saved fine-tuned model
        
    Raises:
        ValueError: If model_name is not supported
        FileNotFoundError: If dataset_path doesn't exist
        
    Example:
        >>> model_path = fine_tune_model(
        ...     "mistralai/Mistral-7B-v0.1",
        ...     "data/polish_instructions.json",
        ...     "checkpoints/wronai-7b",
        ...     learning_rate=1e-4,
        ...     epochs=5
        ... )
        >>> print(f"Model saved to: {model_path}")
    """
```

## ğŸ—‚ï¸ Struktura projektu

DodajÄ…c nowe pliki, zachowaj organizacjÄ™:

```
WronAI/
â”œâ”€â”€ wronai/                    # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                # Model architectures
â”‚   â”œâ”€â”€ training/              # Training utilities
â”‚   â”œâ”€â”€ data/                  # Data processing
â”‚   â””â”€â”€ utils/                 # Common utilities
â”œâ”€â”€ scripts/                   # CLI scripts
â”œâ”€â”€ configs/                   # Configuration files
â”œâ”€â”€ tests/                     # Test files
â”‚   â”œâ”€â”€ unit/                  # Unit tests
â”‚   â”œâ”€â”€ integration/           # Integration tests
â”‚   â””â”€â”€ fixtures/              # Test data
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ notebooks/                 # Jupyter examples
â””â”€â”€ tools/                     # Development tools
```

## ğŸ§ª Testowanie

### Unit Tests
```python
# tests/unit/test_tokenizer.py
import pytest
from wronai.data.tokenizer import PolishTokenizer

class TestPolishTokenizer:
    def test_tokenize_polish_text(self):
        tokenizer = PolishTokenizer()
        text = "Witaj Å›wiecie! Jak siÄ™ masz?"
        tokens = tokenizer.tokenize(text)
        
        assert len(tokens) > 0
        assert all(isinstance(token, str) for token in tokens)
    
    def test_empty_text_handling(self):
        tokenizer = PolishTokenizer()
        
        with pytest.raises(ValueError):
            tokenizer.tokenize("")
```

### Integration Tests
```python
# tests/integration/test_training.py
def test_full_training_pipeline():
    """Test complete training workflow with minimal data."""
    config = {
        "model": {"name": "microsoft/DialoGPT-small"},
        "training": {"num_train_epochs": 1}
    }
    
    # Run minimal training
    result = train_model(config)
    
    assert result.success
    assert os.path.exists(result.model_path)
```

## ğŸ“Š Performance Guidelines

### Memory Optimization
- UÅ¼ywaj gradient checkpointing dla duÅ¼ych modeli
- Implementuj data streaming dla ogromnych datasetÃ³w
- Monitoruj memory usage w testach

```python
# PrzykÅ‚ad memory-efficient data loading
def load_dataset_streaming(path: str, batch_size: int = 1000):
    """Load large dataset in streaming mode."""
    for chunk in pd.read_csv(path, chunksize=batch_size):
        yield process_chunk(chunk)
```

### Training Best Practices
- Zapisuj checkpoints regularnie
- UÅ¼ywaj mixed precision (fp16/bf16)
- Implementuj early stopping
- Loguj metryki do wandb/tensorboard

## ğŸŒ Internationalization

### Polskie komentarze w kodzie
```python
# Preferujemy angielskie komentarze w kodzie
# ale polskie sÄ… OK w dokumentacji uÅ¼ytkownika

# Dobrze: English technical comments
def preprocess_text(text: str) -> str:
    # Remove extra whitespace and normalize Polish diacritics
    return normalize_polish_chars(text.strip())

# OK: Polish comments for domain-specific logic
def detect_polish_sentiment(text: str) -> float:
    # Analiza sentymentu dla jÄ™zyka polskiego
    # uwzglÄ™dniajÄ…ca specyfikÄ™ fleksji
    pass
```

## ğŸ† Recognition

### Contributors
Wszyscy contributors bÄ™dÄ… dodani do:
- README.md Contributors section
- CONTRIBUTORS.md z opisem wkÅ‚adu
- Release notes dla znaczÄ…cych zmian

### Levels of Contribution
- ğŸ¥‰ **Bronze**: 1-5 merged PRs
- ğŸ¥ˆ **Silver**: 6-15 merged PRs + significant feature
- ğŸ¥‡ **Gold**: 16+ merged PRs + major contribution
- ğŸ’ **Diamond**: Core team member

## ğŸ“ Komunikacja

### Channels
- **GitHub Issues**: Bug reports, feature requests
- **GitHub Discussions**: Questions, ideas, showcases
- **Discord**: Real-time chat [Join here](https://discord.gg/wronai)
- **Email**: wronai@example.com dla prywatnych spraw

### Response Times
- Bug reports: 24-48 godzin
- Feature requests: 3-7 dni
- PRs: 2-5 dni roboczych
- Security issues: Natychmiast

## ğŸš« Co nie jest akceptowane

- Kod naruszajÄ…cy prawa autorskie
- Modele trenowane na danych bez odpowiednich licencji
- Implementacje ktÃ³re promujÄ… hate speech
- Kod bez testÃ³w dla krytycznych funkcji
- PR bez opisu zmian
- Commitsy bez opisowych messages

## â“ FAQ

**Q: Czy mogÄ™ dodaÄ‡ support dla innego jÄ™zyka sÅ‚owiaÅ„skiego?**
A: Tak! ChÄ™tnie przyjmiemy wsparcie dla czeskiego, sÅ‚owackiego, ukraiÅ„skiego itp.

**Q: Jak dÅ‚ugo trwa review PR?**
A: Zwykle 2-5 dni roboczych, zaleÅ¼nie od zÅ‚oÅ¼onoÅ›ci.

**Q: Czy mogÄ™ uÅ¼ywaÄ‡ WronAI do celÃ³w komercyjnych?**
A: Tak, projekt jest na licencji Apache 2.0.

**Q: Jak zostaÄ‡ core contributor?**
A: AktywnoÅ›Ä‡ w community, wysokiej jakoÅ›ci contributions, pomoc innym.

---

DziÄ™kujemy za wkÅ‚ad w rozwÃ³j polskiej AI! ğŸ‡µğŸ‡±ğŸ¤–

**Happy coding!** ğŸ’»âœ¨