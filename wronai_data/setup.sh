#!/bin/bash
# setup_wronai_fixed.sh
# Naprawiony setup script dla WronAI

echo "üê¶‚Äç‚¨õ Konfiguracja ≈õrodowiska WronAI Data Collection"

# Sprawd≈∫ czy Python jest zainstalowany
if ! command -v python3 &> /dev/null; then
    echo "‚ùå Python 3 nie jest zainstalowany!"
    exit 1
fi

# Utw√≥rz ≈õrodowisko wirtualne
echo "üì¶ Tworzenie ≈õrodowiska wirtualnego..."
python3 -m venv wronai_env
source wronai_env/bin/activate

# Upgrade pip
echo "‚¨ÜÔ∏è Aktualizacja pip..."
pip install --upgrade pip

# Utw√≥rz requirements.txt
cat > requirements.txt << EOF
# Core dependencies
datasets>=2.14.0
huggingface_hub>=0.16.0
transformers>=4.30.0

# Web scraping and requests
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# Data processing
pandas>=2.0.0
pyarrow>=12.0.0
orjson>=3.9.0

# Progress and CLI
tqdm>=4.65.0
rich>=13.0.0

# Text processing
ftfy>=6.1.0
regex>=2023.6.3
unidecode>=1.3.0

# Compression
zstandard>=0.21.0

# Optional: Language detection (je≈õli chcesz precyzyjniejszƒÖ detekcjƒô)
# langdetect>=1.0.9

# Development tools
pytest>=7.4.0
black>=23.0.0
flake8>=6.0.0
EOF

# Zainstaluj zale≈ºno≈õci
echo "üì• Instalowanie zale≈ºno≈õci..."
pip install -r requirements.txt

# Utw√≥rz strukturƒô projektowƒÖ
echo "üìÅ Tworzenie struktury projektowej..."
mkdir -p {data,scripts,tests,docs}

# Skopiuj naprawiony skrypt
cat > scripts/collect_wronai_data_fixed.py << 'EOF'
#!/usr/bin/env python3
"""
WronAI Data Collection Pipeline - Naprawiona wersja
U≈ºywa dostƒôpnych ≈∫r√≥de≈Ç danych bez wymagania specjalnego dostƒôpu
"""

import os
import json
import logging
from pathlib import Path
from datetime import datetime
import requests
from datasets import load_dataset
from tqdm import tqdm
import re
import time
import ftfy

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WronAICollector:
    def __init__(self, output_dir="./data", target_size_gb=5):
        self.output_dir = Path(output_dir)
        self.target_size_gb = target_size_gb
        self.target_size_bytes = target_size_gb * 1024 * 1024 * 1024
        self.total_size_bytes = 0
        self.source_stats = {}

        self.output_dir.mkdir(exist_ok=True)

    def collect_wikipedia_polish(self):
        """Pobierz polskƒÖ Wikipediƒô."""
        logger.info("Pobieranie polskiej Wikipedii...")

        try:
            # U≈ºywamy dostƒôpnej wersji 2022
            dataset = load_dataset("wikipedia", "20220301.pl", split="train", trust_remote_code=True)

            output_file = self.output_dir / "wikipedia_pl.jsonl"
            processed_count = 0

            with open(output_file, 'w', encoding='utf-8') as f:
                for article in tqdm(dataset, desc="Wikipedia"):
                    if self.total_size_bytes >= self.target_size_bytes:
                        break

                    text = self.clean_text(article['text'])
                    if len(text) < 500:
                        continue

                    item = {
                        'id': f"wiki_{processed_count}",
                        'title': article['title'],
                        'text': text,
                        'source': 'wikipedia'
                    }

                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    processed_count += 1
                    self.total_size_bytes += len(text.encode('utf-8'))

            self.source_stats['wikipedia'] = processed_count
            logger.info(f"Wikipedia: {processed_count} artyku≈Ç√≥w")

        except Exception as e:
            logger.error(f"B≈ÇƒÖd Wikipedia: {e}")

    def collect_wolne_lektury(self):
        """Pobierz Wolne Lektury z lepszym error handling."""
        logger.info("Pobieranie Wolnych Lektur...")

        try:
            api_url = "https://wolnelektury.pl/api/books/"
            response = requests.get(api_url, timeout=30)
            response.raise_for_status()
            books = response.json()

            output_file = self.output_dir / "wolne_lektury.jsonl"
            processed_count = 0

            with open(output_file, 'w', encoding='utf-8') as f:
                for book in tqdm(books[:100], desc="Wolne Lektury"):  # Ograniczamy do 100
                    if self.total_size_bytes >= self.target_size_bytes:
                        break

                    try:
                        # Pobierz szczeg√≥≈Çy ksiƒÖ≈ºki
                        if 'href' in book:
                            detail_response = requests.get(book['href'], timeout=15)
                            if detail_response.status_code == 200:
                                book_detail = detail_response.json()
                                txt_url = book_detail.get('txt')

                                if txt_url:
                                    text_response = requests.get(txt_url, timeout=20)
                                    if text_response.status_code == 200:
                                        text = self.clean_text(text_response.text)

                                        if len(text) > 1000:
                                            item = {
                                                'id': f"lektura_{processed_count}",
                                                'title': book.get('title', ''),
                                                'text': text,
                                                'source': 'wolne_lektury'
                                            }

                                            f.write(json.dumps(item, ensure_ascii=False) + '\n')
                                            processed_count += 1
                                            self.total_size_bytes += len(text.encode('utf-8'))

                        time.sleep(1)  # Respectful crawling

                    except Exception as e:
                        logger.warning(f"B≈ÇƒÖd ksiƒÖ≈ºki {book.get('title', '')}: {e}")
                        continue

            self.source_stats['wolne_lektury'] = processed_count
            logger.info(f"Wolne Lektury: {processed_count} ksiƒÖ≈ºek")

        except Exception as e:
            logger.error(f"B≈ÇƒÖd Wolne Lektury: {e}")

    def collect_alternative_sources(self):
        """Pobierz z alternatywnych ≈∫r√≥de≈Ç."""
        logger.info("Pobieranie alternatywnych ≈∫r√≥de≈Ç...")

        # Pr√≥buj CC-100
        try:
            dataset = load_dataset("cc100", lang="pl", split="train", streaming=True)
            processed_count = 0
            output_file = self.output_dir / "cc100_pl.jsonl"

            with open(output_file, 'w', encoding='utf-8') as f:
                for example in tqdm(dataset, desc="CC-100", total=1000):
                    if processed_count >= 1000 or self.total_size_bytes >= self.target_size_bytes:
                        break

                    text = self.clean_text(example['text'])
                    if len(text) > 200 and self.is_polish(text):
                        item = {
                            'id': f"cc100_{processed_count}",
                            'text': text,
                            'source': 'cc100'
                        }

                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
                        processed_count += 1
                        self.total_size_bytes += len(text.encode('utf-8'))

            self.source_stats['cc100'] = processed_count
            logger.info(f"CC-100: {processed_count} tekst√≥w")

        except Exception as e:
            logger.warning(f"CC-100 niedostƒôpny: {e}")

    def clean_text(self, text):
        """Podstawowe czyszczenie tekstu."""
        if not text:
            return ""

        text = ftfy.fix_text(text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
        text = re.sub(r'<[^>]+>', '', text)

        return text.strip()

    def is_polish(self, text):
        """Prosta detekcja polskiego tekstu."""
        polish_words = ['≈ºe', 'siƒô', 'nie', 'jest', 'jako', 'przez', 'tylko']
        text_lower = text.lower()
        return sum(1 for word in polish_words if word in text_lower) >= 2

    def run(self):
        """Uruchom ca≈Çy pipeline."""
        logger.info(f"Rozpoczynam zbieranie danych (cel: {self.target_size_gb}GB)")

        self.collect_wikipedia_polish()
        if self.total_size_bytes < self.target_size_bytes:
            self.collect_wolne_lektury()
        if self.total_size_bytes < self.target_size_bytes:
            self.collect_alternative_sources()

        logger.info(f"Zako≈Ñczono! Zebrano {self.total_size_bytes/(1024**3):.2f}GB")
        logger.info(f"Statystyki: {self.source_stats}")

if __name__ == "__main__":
    collector = WronAICollector(output_dir="./data", target_size_gb=5)
    collector.run()
EOF

# Utw√≥rz skrypt testowy
cat > scripts/test_data_collection.py << 'EOF'
#!/usr/bin/env python3
"""
Test script dla WronAI data collection
"""

import sys
import logging
from pathlib import Path

# Dodaj scripts do path
sys.path.append(str(Path(__file__).parent))

from collect_wronai_data_fixed import WronAICollector

def test_small_collection():
    """Test z ma≈Çym rozmiarem danych."""
    print("üß™ Test zbierania danych...")

    collector = WronAICollector(
        output_dir="./test_data",
        target_size_gb=0.1  # 100MB dla testu
    )

    collector.run()

    # Sprawd≈∫ wyniki
    data_dir = Path("./test_data")
    files = list(data_dir.glob("*.jsonl"))

    print(f"‚úÖ Utworzono {len(files)} plik√≥w:")
    for file in files:
        size_mb = file.stat().st_size / (1024 * 1024)
        print(f"  - {file.name}: {size_mb:.2f}MB")

if __name__ == "__main__":
    test_small_collection()
EOF

# Utw√≥rz dokumentacjƒô
cat > docs/README.md << 'EOF'
# WronAI Data Collection - Naprawiona Wersja

## PrzeglƒÖd

Ta wersja naprawia problemy z oryginalnym skryptem:

‚úÖ **Naprawione problemy:**
- U≈ºywa dostƒôpnej wersji Wikipedii (20220301.pl)
- Lepszy error handling dla Wolnych Lektur
- Alternatywne ≈∫r√≥d≈Ça zamiast niedostƒôpnego OSCAR
- Prosta deduplikacja hash-based
- Graceful degradation gdy ≈∫r√≥d≈Ça sƒÖ niedostƒôpne

## Szybki start

```bash
# Setup
bash setup_wronai_fixed.sh
source wronai_env/bin/activate

# Test (100MB)
python scripts/test_data_collection.py

# Pe≈Çne zbieranie (5GB)
python scripts/collect_wronai_data_fixed.py
```

## Dostƒôpne ≈∫r√≥d≈Ça

1. **Wikipedia PL** (20220301) - ~1.5M artyku≈Ç√≥w
2. **Wolne Lektury** - literatura klasyczna
3. **CC-100 PL** - korpus internetowy (je≈õli dostƒôpny)
4. **Fallback sources** - dodatkowe ≈∫r√≥d≈Ça

## Struktura danych

```
data/
‚îú‚îÄ‚îÄ wikipedia_pl.jsonl      # Artyku≈Çy Wikipedia
‚îú‚îÄ‚îÄ wolne_lektury.jsonl     # KsiƒÖ≈ºki
‚îî‚îÄ‚îÄ cc100_pl.jsonl          # Teksty internetowe
```

Format JSONL:
```json
{
  "id": "wiki_12345",
  "title": "Tytu≈Ç artyku≈Çu",
  "text": "Tre≈õƒá...",
  "source": "wikipedia"
}
```

## Monitoring

- Logi w konsoli z progress bars
- Statystyki na ko≈Ñcu wykonania
- Graceful stop przy osiƒÖgniƒôciu limitu rozmiaru

## RozwiƒÖzywanie problem√≥w

**Problem**: B≈ÇƒÖd dostƒôpu do datasetu
**RozwiƒÖzanie**: Skrypt automatycznie przechodzi do nastƒôpnego ≈∫r√≥d≈Ça

**Problem**: Timeout przy pobieraniu
**RozwiƒÖzanie**: Zwiƒôkszone timeout'y i retry logic

**Problem**: Brak polskich znak√≥w
**RozwiƒÖzanie**: Prostsza heurystyka detekcji jƒôzyka
EOF

# Utw√≥rz Makefile dla wygody
cat > Makefile << 'EOF'
.PHONY: setup test collect clean

setup:
	bash setup_wronai_fixed.sh

test:
	source wronai_env/bin/activate && python scripts/test_data_collection.py

collect:
	source wronai_env/bin/activate && python scripts/collect_wronai_data_fixed.py

clean:
	rm -rf data/ test_data/ wronai_env/

help:
	@echo "WronAI Data Collection Commands:"
	@echo "  setup   - Setup environment"
	@echo "  test    - Run test collection (100MB)"
	@echo "  collect - Run full collection (5GB)"
	@echo "  clean   - Clean all data and env"
EOF

echo "‚úÖ Setup zako≈Ñczony!"
echo ""
echo "üöÄ Nastƒôpne kroki:"
echo "1. source wronai_env/bin/activate"
echo "2. make test  # Test z 100MB danych"
echo "3. make collect  # Pe≈Çne zbieranie 5GB"
echo ""
echo "üìö Dokumentacja: docs/README.md"