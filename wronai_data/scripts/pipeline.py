#!/usr/bin/env python3
"""
WronAI Master Pipeline
Orchestruje caÅ‚y proces: zbieranie danych â†’ przetwarzanie â†’ trening â†’ inferowanie
"""

import os
import sys
import json
import logging
import subprocess
from pathlib import Path
from datetime import datetime
import argparse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class WronAIMasterPipeline:
    """GÅ‚Ã³wny pipeline do zarzÄ…dzania caÅ‚ym procesem WronAI."""

    def __init__(self, base_dir: str = "./wronai_workspace"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)

        # ÅšcieÅ¼ki do poszczegÃ³lnych etapÃ³w
        self.data_dir = self.base_dir / "data"
        self.processed_dir = self.base_dir / "processed"
        self.model_dir = self.base_dir / "model"
        self.logs_dir = self.base_dir / "logs"

        # UtwÃ³rz strukturÄ™ katalogÃ³w
        for dir_path in [self.data_dir, self.processed_dir, self.model_dir, self.logs_dir]:
            dir_path.mkdir(exist_ok=True)

        # Stan pipeline'u
        self.pipeline_state = {
            'data_collection': False,
            'data_processing': False,
            'model_training': False,
            'model_ready': False
        }

        self.load_pipeline_state()

    def load_pipeline_state(self):
        """ZaÅ‚aduj stan pipeline'u."""
        state_file = self.base_dir / "pipeline_state.json"

        if state_file.exists():
            with open(state_file, 'r') as f:
                self.pipeline_state = json.load(f)
            logger.info("ğŸ“‹ Stan pipeline'u zaÅ‚adowany")
        else:
            logger.info("ğŸ“‹ Nowy pipeline - stan resetowany")

    def save_pipeline_state(self):
        """Zapisz stan pipeline'u."""
        self.pipeline_state['last_updated'] = datetime.now().isoformat()

        state_file = self.base_dir / "pipeline_state.json"
        with open(state_file, 'w') as f:
            json.dump(self.pipeline_state, f, indent=2)

    def check_dependencies(self):
        """SprawdÅº czy wszystkie zaleÅ¼noÅ›ci sÄ… zainstalowane."""
        logger.info("ğŸ” Sprawdzanie zaleÅ¼noÅ›ci...")

        required_packages = [
            'torch', 'transformers', 'datasets', 'peft',
            'bitsandbytes', 'accelerate', 'gradio'
        ]

        missing = []
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing.append(package)

        if missing:
            logger.error(f"âŒ BrakujÄ…ce pakiety: {', '.join(missing)}")
            logger.info("Zainstaluj przez: pip install -r requirements.txt")
            return False

        logger.info("âœ… Wszystkie zaleÅ¼noÅ›ci dostÄ™pne")
        return True

    def run_data_collection(self, target_size_mb: int = 500):
        """Uruchom zbieranie danych."""
        logger.info("ğŸ“¥ Rozpoczynam zbieranie danych...")

        try:
            # Import i uruchomienie data collection
            from quick_start_wronai import SimpleWronAICollector

            collector = SimpleWronAICollector(target_size_mb=target_size_mb)
            summary = collector.run()

            # PrzenieÅ› dane do workspace
            import shutil
            source_dir = Path("./wronai_simple_data")
            if source_dir.exists():
                if self.data_dir.exists():
                    shutil.rmtree(self.data_dir)
                shutil.move(str(source_dir), str(self.data_dir))

            self.pipeline_state['data_collection'] = True
            self.save_pipeline_state()

            logger.info(f"âœ… Zbieranie danych zakoÅ„czone: {summary['actual_size_mb']:.1f}MB")
            return summary

        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d zbierania danych: {e}")
            return None

    def run_data_processing(self):
        """Uruchom przetwarzanie danych."""
        if not self.pipeline_state['data_collection']:
            logger.error("âŒ Najpierw zbierz dane!")
            return None

        logger.info("âš™ï¸ Rozpoczynam przetwarzanie danych...")

        try:
            # Import i uruchomienie data processing
            sys.path.append(str(Path(__file__).parent))
            from wronai_data_processing import WronAIDataProcessor

            processor = WronAIDataProcessor(
                input_dir=str(self.data_dir),
                output_dir=str(self.processed_dir)
            )

            report = processor.run_processing_pipeline()

            self.pipeline_state['data_processing'] = True
            self.save_pipeline_state()

            logger.info(f"âœ… Przetwarzanie zakoÅ„czone: {report['statistics']['total_chunks']} chunkÃ³w")
            return report

        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d przetwarzania: {e}")
            return None

    def run_model_training(self, model_name: str = "microsoft/DialoGPT-medium"):
        """Uruchom trening modelu."""
        if not self.pipeline_state['data_processing']:
            logger.error("âŒ Najpierw przetwÃ³rz dane!")
            return None

        logger.info("ğŸ‹ï¸ Rozpoczynam trening modelu...")

        try:
            from wronai_training import WronAITrainer

            trainer = WronAITrainer(
                model_name=model_name,
                data_dir=str(self.processed_dir),
                output_dir=str(self.model_dir)
            )

            report = trainer.run_full_training_pipeline()

            self.pipeline_state['model_training'] = True
            self.pipeline_state['model_ready'] = True
            self.save_pipeline_state()

            logger.info(f"âœ… Trening zakoÅ„czony: {report['training_stats']['train_loss']:.4f} loss")
            return report

        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d treningu: {e}")
            return None

    def run_model_inference(self):
        """Uruchom inferowanie modelu."""
        if not self.pipeline_state['model_ready']:
            logger.error("âŒ Najpierw wytrenuj model!")
            return None

        logger.info("ğŸ¤– Uruchamiam inferowanie...")

        try:
            from wronai_inference import WronAIInference

            inference = WronAIInference(model_path=str(self.model_dir))

            # Podstawowa ewaluacja
            eval_results = inference.evaluate_polish_capabilities()
            benchmark_results = inference.benchmark_performance()

            logger.info("âœ… Model gotowy do uÅ¼ycia!")
            return inference, eval_results, benchmark_results

        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d inferowania: {e}")
            return None

    def run_full_pipeline(self, target_size_mb: int = 500, model_name: str = "microsoft/DialoGPT-medium"):
        """Uruchom kompletny pipeline."""
        logger.info("ğŸš€ Rozpoczynam peÅ‚ny pipeline WronAI...")

        pipeline_results = {}

        # 1. Zbieranie danych
        if not self.pipeline_state['data_collection']:
            logger.info("ğŸ“¥ Etap 1/4: Zbieranie danych")
            data_summary = self.run_data_collection(target_size_mb)
            if not data_summary:
                return None
            pipeline_results['data_collection'] = data_summary
        else:
            logger.info("â­ï¸ Etap 1/4: Dane juÅ¼ zebrane")

        # 2. Przetwarzanie danych
        if not self.pipeline_state['data_processing']:
            logger.info("âš™ï¸ Etap 2/4: Przetwarzanie danych")
            processing_report = self.run_data_processing()
            if not processing_report:
                return None
            pipeline_results['data_processing'] = processing_report
        else:
            logger.info("â­ï¸ Etap 2/4: Dane juÅ¼ przetworzone")

        # 3. Trening modelu
        if not self.pipeline_state['model_training']:
            logger.info("ğŸ‹ï¸ Etap 3/4: Trening modelu")
            training_report = self.run_model_training(model_name)
            if not training_report:
                return None
            pipeline_results['model_training'] = training_report
        else:
            logger.info("â­ï¸ Etap 3/4: Model juÅ¼ wytrenowany")

        # 4. Inferowanie
        logger.info("ğŸ¤– Etap 4/4: Testowanie modelu")
        inference_results = self.run_model_inference()
        if inference_results:
            pipeline_results['inference'] = {
                'evaluation': inference_results[1],
                'benchmark': inference_results[2]
            }

        # Zapisz finalny raport
        final_report = {
            'pipeline_completed': datetime.now().isoformat(),
            'workspace': str(self.base_dir),
            'results': pipeline_results,
            'state': self.pipeline_state
        }

        report_file = self.base_dir / "final_pipeline_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)

        logger.info("ğŸ‰ Pipeline WronAI zakoÅ„czony pomyÅ›lnie!")
        logger.info(f"ğŸ“ Workspace: {self.base_dir}")
        logger.info(f"ğŸ“Š Raport: {report_file}")

        return final_report

    def show_status(self):
        """PokaÅ¼ status pipeline'u."""
        print("\nğŸ¦â€â¬› Status Pipeline WronAI")
        print("=" * 40)
        print(f"ğŸ“ Workspace: {self.base_dir}")
        print(f"ğŸ“Š Stan:")

        status_icons = {True: "âœ…", False: "âŒ"}

        for step, completed in self.pipeline_state.items():
            if step != 'last_updated':
                icon = status_icons[completed]
                step_name = step.replace('_', ' ').title()
                print(f"  {icon} {step_name}")

        if 'last_updated' in self.pipeline_state:
            print(f"ğŸ•’ Ostatnia aktualizacja: {self.pipeline_state['last_updated']}")

        print()

    def clean_workspace(self):
        """WyczyÅ›Ä‡ workspace."""
        import shutil

        response = input(f"âš ï¸ Czy na pewno usunÄ…Ä‡ workspace {self.base_dir}? (y/N): ")
        if response.lower() == 'y':
            shutil.rmtree(self.base_dir)
            logger.info("ğŸ—‘ï¸ Workspace wyczyszczony")
        else:
            logger.info("Anulowano czyszczenie")


def create_requirements_file():
    """UtwÃ³rz plik requirements.txt."""
    requirements = """# WronAI - Wymagania systemowe

# Core ML libraries
torch>=2.0.0
transformers>=4.30.0
datasets>=2.14.0
tokenizers>=0.13.0

# Training optimizations
peft>=0.4.0
bitsandbytes>=0.41.0
accelerate>=0.20.0

# Data processing
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=12.0.0

# Web scraping & HTTP
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# Text processing
ftfy>=6.1.0
regex>=2023.6.3
unidecode>=1.3.0

# Progress & CLI
tqdm>=4.65.0
rich>=13.0.0

# Interface
gradio>=3.40.0

# Logging & monitoring
tensorboard>=2.13.0
wandb>=0.15.0

# JSON handling
orjson>=3.9.0

# Compression
zstandard>=0.21.0

# Development
pytest>=7.4.0
black>=23.0.0
flake8>=6.0.0
"""

    with open("requirements.txt", 'w') as f:
        f.write(requirements)

    logger.info("ğŸ“ Utworzono requirements.txt")


def main():
    """GÅ‚Ã³wna funkcja CLI."""
    parser = argparse.ArgumentParser(description="WronAI Master Pipeline")
    parser.add_argument("--workspace", default="./wronai_workspace", help="ÅšcieÅ¼ka do workspace")
    parser.add_argument("--size", type=int, default=500, help="Rozmiar danych w MB")
    parser.add_argument("--model", default="microsoft/DialoGPT-medium", help="Model bazowy")

    subparsers = parser.add_subparsers(dest="command", help="DostÄ™pne komendy")

    # Komendy
    subparsers.add_parser("setup", help="Setup Å›rodowiska")
    subparsers.add_parser("collect", help="Zbierz dane")
    subparsers.add_parser("process", help="PrzetwÃ³rz dane")
    subparsers.add_parser("train", help="Trenuj model")
    subparsers.add_parser("infer", help="Uruchom inferowanie")
    subparsers.add_parser("full", help="PeÅ‚ny pipeline")
    subparsers.add_parser("status", help="Status pipeline")
    subparsers.add_parser("clean", help="WyczyÅ›Ä‡ workspace")

    args = parser.parse_args()

    # UtwÃ³rz pipeline
    pipeline = WronAIMasterPipeline(args.workspace)

    print("ğŸ¦â€â¬› WronAI Master Pipeline")
    print("=" * 50)

    if args.command == "setup":
        create_requirements_file()
        print("ğŸ”§ Setup zakoÅ„czony!")
        print("NastÄ™pne kroki:")
        print("1. pip install -r requirements.txt")
        print("2. python wronai_master_pipeline.py full")

    elif args.command == "collect":
        if pipeline.check_dependencies():
            pipeline.run_data_collection(args.size)

    elif args.command == "process":
        if pipeline.check_dependencies():
            pipeline.run_data_processing()

    elif args.command == "train":
        if pipeline.check_dependencies():
            pipeline.run_model_training(args.model)

    elif args.command == "infer":
        if pipeline.check_dependencies():
            results = pipeline.run_model_inference()
            if results:
                inference, eval_results, benchmark = results

                print(f"\nğŸ“Š Wyniki ewaluacji:")
                success_rate = (eval_results['passed_tests'] / eval_results['total_tests']) * 100
                print(f"  - Testy: {success_rate:.1f}% zaliczonych")
                print(f"  - WydajnoÅ›Ä‡: {benchmark['tokens_per_second']:.1f} tokenÃ³w/s")

                # Interaktywny tryb
                response = input("\nUruchomiÄ‡ interfejs Gradio? (y/N): ")
                if response.lower() == 'y':
                    interface = inference.create_gradio_interface()
                    interface.launch(share=True)

    elif args.command == "full":
        if pipeline.check_dependencies():
            pipeline.run_full_pipeline(args.size, args.model)

    elif args.command == "status":
        pipeline.show_status()

    elif args.command == "clean":
        pipeline.clean_workspace()

    else:
        print("UÅ¼ycie:")
        print("  python wronai_master_pipeline.py setup    # Setup Å›rodowiska")
        print("  python wronai_master_pipeline.py full     # PeÅ‚ny pipeline")
        print("  python wronai_master_pipeline.py status   # Status")
        print("  python wronai_master_pipeline.py infer    # Inferowanie")
        print()
        print("Opcje:")
        print("  --workspace DIR    # Katalog roboczy")
        print("  --size MB          # Rozmiar danych")
        print("  --model NAME       # Model bazowy")


if __name__ == "__main__":
    main()