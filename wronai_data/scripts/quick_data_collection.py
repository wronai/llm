#!/usr/bin/env python3
"""
WronAI Quick Start - Minimal working version
ProstÄ… wersja do szybkiego uruchomienia bez problemÃ³w z dostÄ™pem do danych
"""

import json
import logging
from pathlib import Path
from datetime import datetime
import requests
from datasets import load_dataset
from tqdm import tqdm
import re
import time

# Konfiguracja
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SimpleWronAICollector:
    """Uproszczona wersja collectora bez zewnÄ™trznych zaleÅ¼noÅ›ci."""

    def __init__(self, target_size_mb=500):  # DomyÅ›lnie 500MB
        self.output_dir = Path("./wronai_simple_data")
        self.target_size_bytes = target_size_mb * 1024 * 1024
        self.total_size = 0
        self.stats = {}

        self.output_dir.mkdir(exist_ok=True)
        logger.info(f"Cel: {target_size_mb}MB danych w {self.output_dir}")

    def clean_text(self, text):
        """Podstawowe czyszczenie."""
        if not text:
            return ""

        # UsuÅ„ nadmiar whitespace
        text = re.sub(r'\s+', ' ', text)
        # UsuÅ„ HTML tagi
        text = re.sub(r'<[^>]+>', '', text)
        # UsuÅ„ URLe
        text = re.sub(r'http\S+', '', text)

        return text.strip()

    def is_polish_simple(self, text):
        """Bardzo prosta detekcja polskiego."""
        polish_indicators = ['Å¼e', 'siÄ™', 'nie', 'jest', 'jako', 'przez', 'tylko', 'moÅ¼e', 'oraz']
        text_lower = text.lower()
        score = sum(1 for word in polish_indicators if word in text_lower)
        return score >= 2

    def collect_wikipedia(self):
        """Pobierz polskÄ… WikipediÄ™ - wersja ktÃ³ra dziaÅ‚a."""
        logger.info("ðŸ“– Pobieranie Wikipedia...")

        try:
            # UÅ¼yj wersji ktÃ³ra na pewno istnieje
            dataset = load_dataset("wikipedia", "20220301.pl", split="train", trust_remote_code=True)

            output_file = self.output_dir / "wikipedia.jsonl"
            count = 0

            with open(output_file, 'w', encoding='utf-8') as f:
                for article in tqdm(dataset, desc="Wikipedia articles"):
                    if self.total_size >= self.target_size_bytes:
                        break

                    text = self.clean_text(article['text'])
                    if len(text) < 300:  # PomiÅ„ krÃ³tkie
                        continue

                    item = {
                        'id': f"wiki_{count}",
                        'title': article['title'],
                        'text': text,
                        'source': 'wikipedia',
                        'timestamp': datetime.now().isoformat()
                    }

                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    count += 1
                    self.total_size += len(text.encode('utf-8'))

                    if count % 1000 == 0:
                        logger.info(f"Wikipedia: {count} artykuÅ‚Ã³w, {self.total_size / 1024 / 1024:.1f}MB")

            self.stats['wikipedia'] = count
            logger.info(f"âœ… Wikipedia: {count} artykuÅ‚Ã³w zebrano")

        except Exception as e:
            logger.error(f"âŒ Wikipedia error: {e}")
            self.stats['wikipedia'] = 0

    def collect_synthetic_polish(self):
        """Generuj syntetyczne polskie teksty jako fallback."""
        logger.info("ðŸ”§ Generowanie syntetycznych przykÅ‚adÃ³w...")

        # Podstawowe polskie teksty jako starter
        polish_samples = [
            "JÄ™zyk polski naleÅ¼y do grupy zachodniej jÄ™zykÃ³w sÅ‚owiaÅ„skich. Jest jÄ™zykiem urzÄ™dowym w Polsce.",
            "Historia Polski siÄ™ga X wieku, kiedy to powstaÅ‚o pierwsze paÅ„stwo polskie pod rzÄ…dami Mieszka I.",
            "Warszawa jest stolicÄ… Polski od 1596 roku. Miasto leÅ¼y nad WisÅ‚Ä… w wojewÃ³dztwie mazowieckim.",
            "Literatura polska ma bogate tradycje. NajsÅ‚ynniejszymi poetami sÄ… Adam Mickiewicz i Juliusz SÅ‚owacki.",
            "Polskie tradycje kulinarne obejmujÄ… pierogi, bigos, kotlet schabowy i wiele innych potraw.",
            "System edukacji w Polsce skÅ‚ada siÄ™ ze szkoÅ‚y podstawowej, Å›redniej i wyÅ¼szej.",
            "Polska przystÄ…piÅ‚a do Unii Europejskiej w 2004 roku wraz z dziewiÄ™cioma innymi krajami.",
            "Gospodarka Polski opiera siÄ™ gÅ‚Ã³wnie na przemyÅ›le, usÅ‚ugach i rolnictwie.",
            "Polskie miasta majÄ… bogate dziedzictwo architektoniczne, szczegÃ³lnie KrakÃ³w i GdaÅ„sk.",
            "JÄ™zyk polski uÅ¼ywa alfabetu Å‚aciÅ„skiego rozszerzonego o polskie znaki diakrytyczne."
        ]

        output_file = self.output_dir / "synthetic.jsonl"
        count = 0

        with open(output_file, 'w', encoding='utf-8') as f:
            # Rozszerz prÃ³bki przez warianty
            for i, base_text in enumerate(polish_samples):
                if self.total_size >= self.target_size_bytes:
                    break

                # UtwÃ³rz kilka wariantÃ³w kaÅ¼dego tekstu
                variants = [
                    base_text,
                    f"WedÅ‚ug ekspertÃ³w, {base_text.lower()}",
                    f"Warto wiedzieÄ‡, Å¼e {base_text.lower()}",
                    f"Jak pokazujÄ… badania, {base_text.lower()}",
                    f"NaleÅ¼y podkreÅ›liÄ‡, Å¼e {base_text.lower()}"
                ]

                for variant in variants:
                    if self.total_size >= self.target_size_bytes:
                        break

                    # Dodaj wiÄ™cej kontekstu
                    extended_text = f"{variant} To sprawia, Å¼e jest to waÅ¼ny element polskiej kultury i toÅ¼samoÅ›ci narodowej. Wiedza na ten temat jest kluczowa dla zrozumienia wspÃ³Å‚czesnej Polski."

                    item = {
                        'id': f"synthetic_{count}",
                        'text': extended_text,
                        'source': 'synthetic',
                        'timestamp': datetime.now().isoformat()
                    }

                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    count += 1
                    self.total_size += len(extended_text.encode('utf-8'))

        self.stats['synthetic'] = count
        logger.info(f"âœ… Synthetic: {count} tekstÃ³w wygenerowano")

    def collect_wolne_lektury_safe(self):
        """Bezpieczne pobieranie Wolnych Lektur z timeout'ami."""
        logger.info("ðŸ“š PrÃ³bujÄ™ pobraÄ‡ Wolne Lektury...")

        try:
            # KrÃ³tki timeout - jeÅ›li nie dziaÅ‚a, nie blokujemy
            response = requests.get("https://wolnelektury.pl/api/books/", timeout=10)

            if response.status_code != 200:
                logger.warning("Wolne Lektury niedostÄ™pne, pomijam")
                return

            books = response.json()[:20]  # Tylko pierwsze 20

            output_file = self.output_dir / "lektury.jsonl"
            count = 0

            with open(output_file, 'w', encoding='utf-8') as f:
                for book in tqdm(books, desc="Wolne Lektury"):
                    if self.total_size >= self.target_size_bytes:
                        break

                    try:
                        if 'href' not in book:
                            continue

                        # Pobierz szczegÃ³Å‚y
                        detail_resp = requests.get(book['href'], timeout=5)
                        if detail_resp.status_code == 200:
                            detail = detail_resp.json()
                            txt_url = detail.get('txt')

                            if txt_url:
                                text_resp = requests.get(txt_url, timeout=10)
                                if text_resp.status_code == 200:
                                    text = self.clean_text(text_resp.text)

                                    if len(text) > 1000:  # Tylko dÅ‚uÅ¼sze teksty
                                        item = {
                                            'id': f"lektura_{count}",
                                            'title': book.get('title', ''),
                                            'text': text[:5000],  # Ogranicz rozmiar
                                            'source': 'wolne_lektury',
                                            'timestamp': datetime.now().isoformat()
                                        }

                                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
                                        count += 1
                                        self.total_size += len(item['text'].encode('utf-8'))

                        time.sleep(0.5)  # BÄ…dÅº grzeczny

                    except Exception as e:
                        logger.warning(f"BÅ‚Ä…d ksiÄ…Å¼ki: {e}")
                        continue

            self.stats['wolne_lektury'] = count
            logger.info(f"âœ… Wolne Lektury: {count} ksiÄ…Å¼ek")

        except Exception as e:
            logger.warning(f"Wolne Lektury caÅ‚kowicie niedostÄ™pne: {e}")
            self.stats['wolne_lektury'] = 0

    def create_summary(self):
        """UtwÃ³rz podsumowanie zebranych danych."""
        summary = {
            'collection_date': datetime.now().isoformat(),
            'target_size_mb': self.target_size_bytes / 1024 / 1024,
            'actual_size_mb': self.total_size / 1024 / 1024,
            'sources': self.stats,
            'total_items': sum(self.stats.values()),
            'files_created': list(self.output_dir.glob("*.jsonl"))
        }

        # Zapisz podsumowanie
        summary_file = self.output_dir / "collection_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            # Konwertuj Path objects do string dla JSON
            summary['files_created'] = [str(f.name) for f in summary['files_created']]
            json.dump(summary, f, ensure_ascii=False, indent=2)

        return summary

    def run(self):
        """Uruchom kompletny proces zbierania."""
        logger.info("ðŸš€ Rozpoczynam zbieranie danych WronAI...")

        # Faza 1: Wikipedia (najwaÅ¼niejsze)
        if self.total_size < self.target_size_bytes:
            self.collect_wikipedia()

        # Faza 2: Wolne Lektury (jeÅ›li dostÄ™pne)
        if self.total_size < self.target_size_bytes:
            self.collect_wolne_lektury_safe()

        # Faza 3: Syntetyczne dane (fallback)
        if self.total_size < self.target_size_bytes:
            self.collect_synthetic_polish()

        # Podsumowanie
        summary = self.create_summary()

        logger.info("ðŸŽ‰ Zbieranie zakoÅ„czone!")
        logger.info(f"ðŸ“Š Zebrano: {summary['actual_size_mb']:.1f}MB z {summary['total_items']} elementÃ³w")
        logger.info(f"ðŸ“ Pliki: {', '.join(summary['files_created'])}")
        logger.info(f"ðŸ“ˆ Å¹rÃ³dÅ‚a: {summary['sources']}")

        return summary


def main():
    """GÅ‚Ã³wna funkcja."""
    print("ðŸ¦â€â¬› WronAI Data Collection - Quick Start")
    print("=" * 50)

    # DomyÅ›lnie zbierz 500MB - wystarczy na testy
    collector = SimpleWronAICollector(target_size_mb=500)

    try:
        summary = collector.run()

        print("\nâœ… SUKCES!")
        print(f"Zebrano {summary['actual_size_mb']:.1f}MB polskich danych")
        print(f"Lokalizacja: {collector.output_dir}")
        print("\nNastÄ™pne kroki:")
        print("1. SprawdÅº pliki .jsonl w katalogu wronai_simple_data/")
        print("2. UÅ¼yj danych do treningu modelu WronAI")
        print("3. ZwiÄ™ksz target_size_mb dla wiÄ™kszego datasetu")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Przerwano przez uÅ¼ytkownika")
    except Exception as e:
        print(f"\nâŒ BÅ‚Ä…d: {e}")
        print("SprawdÅº logi powyÅ¼ej dla szczegÃ³Å‚Ã³w")


if __name__ == "__main__":
    main()