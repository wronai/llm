# WronAI Data Collection Pipeline

System do pobierania i przetwarzania polskich danych treningowych dla modeli jÄ™zykowych.

## Opis projektu

WronAI Data Collection Pipeline to kompleksowe narzÄ™dzie do zbierania, czyszczenia i przetwarzania polskojÄ™zycznych danych tekstowych z rÃ³Å¼nych ÅºrÃ³deÅ‚. System zostaÅ‚ zaprojektowany do tworzenia wysokiej jakoÅ›ci korpusu treningowego dla modeli jÄ™zykowych w jÄ™zyku polskim.

## Å¹rÃ³dÅ‚a danych

Pipeline zbiera dane z nastÄ™pujÄ…cych ÅºrÃ³deÅ‚:

1. **Wysokiej jakoÅ›ci**:
   - Polska Wikipedia
   - Wolne Lektury (polskie ksiÄ…Å¼ki w domenie publicznej)
   - ArtykuÅ‚y akademickie

2. **Åšredniej jakoÅ›ci**:
   - OSCAR Polish (filtrowany Common Crawl)
   - Common Crawl (wybrane polskie domeny)

## FunkcjonalnoÅ›ci

- Pobieranie danych z wielu ÅºrÃ³deÅ‚
- Czyszczenie i normalizacja tekstu
- Deduplikacja na poziomie dokumentÃ³w
- Identyfikacja jÄ™zyka (filtrowanie niepolskich tekstÃ³w)
- Tworzenie podziaÅ‚Ã³w na zbiory treningowe, walidacyjne i testowe
- Generowanie metadanych

## Wymagania systemowe

- Python 3.8 lub nowszy
- DostÄ™p do internetu
- Min. 16GB RAM (zalecane)
- PrzestrzeÅ„ dyskowa: min. 100GB (zaleÅ¼nie od docelowego rozmiaru korpusu)

## Instalacja

```bash
# Sklonuj repozytorium
git clone https://github.com/wronai/llm.git
cd llm/wronai_data

# Uruchom skrypt instalacyjny
bash setup.sh
```

Skrypt `setup.sh` automatycznie:
1. Tworzy wirtualne Å›rodowisko Python
2. Instaluje wszystkie wymagane zaleÅ¼noÅ›ci z pliku requirements.txt
3. Pobiera model FastText do identyfikacji jÄ™zyka

# WronAI Data Collection - Naprawiona Wersja

## PrzeglÄ…d

Ta wersja naprawia problemy z oryginalnym skryptem:

âœ… **Naprawione problemy:**
- UÅ¼ywa dostÄ™pnej wersji Wikipedii (20220301.pl)
- Lepszy error handling dla Wolnych Lektur
- Alternatywne ÅºrÃ³dÅ‚a zamiast niedostÄ™pnego OSCAR
- Prosta deduplikacja hash-based
- Graceful degradation gdy ÅºrÃ³dÅ‚a sÄ… niedostÄ™pne

## Szybki start

```bash
# Setup
bash setup.sh
source venv/bin/activate

# Test (100MB)
python3 scripts/test_data_collection.py

# PeÅ‚ne zbieranie (5GB)
python3 scripts/collect_wronai_data_fixed.py
```

## DostÄ™pne ÅºrÃ³dÅ‚a

1. **Wikipedia PL** (20220301) - ~1.5M artykuÅ‚Ã³w
2. **Wolne Lektury** - literatura klasyczna
3. **CC-100 PL** - korpus internetowy (jeÅ›li dostÄ™pny)
4. **Fallback sources** - dodatkowe ÅºrÃ³dÅ‚a

## Struktura danych

```
data/
â”œâ”€â”€ wikipedia_pl.jsonl      # ArtykuÅ‚y Wikipedia
â”œâ”€â”€ wolne_lektury.jsonl     # KsiÄ…Å¼ki
â””â”€â”€ cc100_pl.jsonl          # Teksty internetowe
```

Format JSONL:
```json
{
  "id": "wiki_12345",
  "title": "TytuÅ‚ artykuÅ‚u",
  "text": "TreÅ›Ä‡...",
  "source": "wikipedia"
}
```

## Monitoring

- Logi w konsoli z progress bars
- Statystyki na koÅ„cu wykonania
- Graceful stop przy osiÄ…gniÄ™ciu limitu rozmiaru

## RozwiÄ…zywanie problemÃ³w

**Problem**: BÅ‚Ä…d dostÄ™pu do datasetu
**RozwiÄ…zanie**: Skrypt automatycznie przechodzi do nastÄ™pnego ÅºrÃ³dÅ‚a

**Problem**: Timeout przy pobieraniu
**RozwiÄ…zanie**: ZwiÄ™kszone timeout'y i retry logic

**Problem**: Brak polskich znakÃ³w
**RozwiÄ…zanie**: Prostsza heurystyka detekcji jÄ™zyka


## UÅ¼ycie

### Uruchamianie skryptÃ³w

WronAI Data Collection Pipeline zawiera kilka skryptÃ³w do zbierania danych. MoÅ¼esz je Å‚atwo uruchamiaÄ‡ za pomocÄ… skryptu `run_scripts.py`:

```bash
# Aktywuj Å›rodowisko wirtualne
source venv/bin/activate

# WyÅ›wietl dostÄ™pne skrypty
python3 run_scripts.py list

# Uruchom peÅ‚ny pipeline zbierania danych
python3 run_scripts.py collect

# Uruchom szybkÄ… wersjÄ™ zbierania danych (mniej zaleÅ¼noÅ›ci)
python3 run_scripts.py quick

# Uruchom testy
python3 run_scripts.py test
```

### Parametry konfiguracyjne

MoÅ¼esz dostosowaÄ‡ parametry uruchomienia:

```python
collector = WronAIDataCollector(
    output_dir="./output_data",  # Katalog wyjÅ›ciowy
    target_size_gb=50            # Docelowy rozmiar korpusu w GB
)
collector.run_collection_pipeline()
```

## Struktura katalogÃ³w

```
wronai_data/
â”œâ”€â”€ raw_data/                # Surowe dane z rÃ³Å¼nych ÅºrÃ³deÅ‚
â”‚   â”œâ”€â”€ high_quality/        # Å¹rÃ³dÅ‚a wysokiej jakoÅ›ci
â”‚   â”‚   â”œâ”€â”€ wikipedia_pl/
â”‚   â”‚   â”œâ”€â”€ wolne_lektury/
â”‚   â”‚   â””â”€â”€ academic_papers/
â”‚   â””â”€â”€ medium_quality/      # Å¹rÃ³dÅ‚a Å›redniej jakoÅ›ci
â”‚       â”œâ”€â”€ oscar_pl/
â”‚       â””â”€â”€ common_crawl/
â”œâ”€â”€ processed_data/          # Przetworzone dane
â”‚   â”œâ”€â”€ filtered/            # Po filtrowaniu jÄ™zyka
â”‚   â”œâ”€â”€ deduplicated/        # Po deduplikacji
â”‚   â””â”€â”€ tokenized/           # Po tokenizacji
â”œâ”€â”€ splits/                  # PodziaÅ‚y na zbiory
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ validation/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ metadata/                # Metadane korpusu
â””â”€â”€ logs/                    # Logi procesu
```
{ÅºrÃ³dÅ‚o}_{jakoÅ›Ä‡}_{domena}_{wersja}_{split}_{shard}.{format}
```

**PrzykÅ‚ady:**
- `wikipedia_high_general_v2.1_train_00001.jsonl`
- `oscar_medium_web_v23.01_val_00042.parquet`
- `nkjp_high_literature_v1.5_test_00001.jsonl`


## Algorytm pobierania w kolejnoÅ›ci priorytetowej

### Strategia Quality-First z balansowaniem domen

**Faza 1: Fundament wysokiej jakoÅ›ci (15GB)**
1. **Wikipedia Polish** (4GB) - pobierz najnowszy dump, wyodrÄ™bnij czyste teksty
2. **Wolne Lektury** (3GB) - uÅ¼yj API do pobrania wszystkich dostÄ™pnych dzieÅ‚
3. **NKJP zbalansowany** (5GB) - pobierz dostÄ™pne podkorpusy
4. **CLARIN-PL akademicki** (2GB) - zbierz wysokiej jakoÅ›ci zbiory specjalistyczne
5. **Parliamentary Corpus** (1GB) - oficjalne transkrypcje

**Faza 2: GÅ‚Ã³wny korpus internetowy (30GB)**
1. **OSCAR Polish deduplicated** (30GB) - pobierz najnowszÄ… wersjÄ™ 23.01
2. Zastosuj dodatkowe filtrowanie jakoÅ›ci (perplexity < 500)
3. Wykonaj deduplikacjÄ™ na poziomie dokumentÃ³w i zdaÅ„

**Faza 3: UzupeÅ‚nienie i balansowanie (5GB)**
1. **Common Crawl CC-100** - dla zwiÄ™kszenia rÃ³Å¼norodnoÅ›ci internetowej
2. **Conversational data** - SpokesBiz i DiaBiz dla jÄ™zyka mÃ³wionego
3. **Specialized domains** - techniczne, naukowe, biznesowe teksty

### Strategie deduplikacji i czyszczenia

**Multi-poziomowa deduplikacja:**
- **Dokument-level**: MinHash LSH z progiem 0.8 (usuwa 70-80% duplikatÃ³w)
- **Zdanie-level**: Usuwanie powtarzajÄ…cych siÄ™ fraz
- **Line-level**: Deduplikacja w ramach losowych bucket'Ã³w

**Pipeline czyszczenia:**
1. **Encoding normalization**: Konwersja do UTF-8, naprawa ftfy
2. **Language detection**: FastText z 95% confidence threshold
3. **Content filtering**: Usuwanie HTML, boilerplate, offensive content
4. **Quality scoring**: Perplexity-based + heuristic features
5. **Length filtering**: Min 100 znakÃ³w, max 50K znakÃ³w na dokument

### Balansowanie domen jÄ™zykowych

**Docelowy rozkÅ‚ad 50GB datasetu:**
- **OSCAR (web content)**: 35GB (70%) - rÃ³Å¼norodnoÅ›Ä‡ internetowa
- **Wikipedia**: 4GB (8%) - wysoka jakoÅ›Ä‡, encyklopedycznoÅ›Ä‡
- **NKJP**: 5GB (10%) - zbalansowana reprezentacja gatunkÃ³w
- **Wolne Lektury**: 3GB (6%) - jÄ™zyk literacki, kultura
- **CLARIN-PL**: 2GB (4%) - domeny specjalistyczne
- **Parliamentary**: 1GB (2%) - jÄ™zyk formalny, polityczny

## Konkretny skrypt do automatycznego pobierania


### Skrypt Docker dla reprodukowalnego Å›rodowiska


## Szacunkowe rozmiary i wymagania

### Rozmiary ÅºrÃ³deÅ‚ danych

**Å¹rÃ³dÅ‚a pierwszego priorytetu (15GB):**
- **Wikipedia Polish**: 4GB nieskompresowany tekst
- **Wolne Lektury**: 3GB literatury wysokiej jakoÅ›ci  
- **NKJP zbalansowany**: 5GB zbalansowanego korpusu
- **CLARIN-PL**: 2GB zbiorÃ³w akademickich
- **Parliamentary Corpus**: 1GB oficjalnych transkrypcji

**GÅ‚Ã³wny korpus (30GB):**
- **OSCAR Polish deduplicated**: 49GB dostÄ™pny, 30GB po dodatkowym filtrowaniu
- **Common Crawl CC-100**: 12GB skompresowany, uzupeÅ‚nienie rÃ³Å¼norodnoÅ›ci

**UzupeÅ‚nienie (5GB):**
- **Conversational datasets**: 1GB rozmÃ³w i dialogÃ³w
- **Specialized domains**: 4GB technicznych i naukowych tekstÃ³w

### Wymagania infrastrukturalne

## RozwiÄ…zywanie problemÃ³w

### Brak dostÄ™pu do OSCAR

JeÅ›li napotkasz problemy z dostÄ™pem do datasetu OSCAR, upewnij siÄ™, Å¼e uÅ¼ywasz publicznie dostÄ™pnej wersji lub masz odpowiednie uprawnienia.

### Problemy z pamiÄ™ciÄ…

Dla duÅ¼ych korpusÃ³w zalecane jest uruchamianie skryptu na maszynie z co najmniej 16GB RAM. MoÅ¼esz zmniejszyÄ‡ parametr `target_size_gb` aby zmniejszyÄ‡ wymagania pamiÄ™ciowe.

## Licencja

Ten projekt jest udostÄ™pniany na licencji Apache 2.0. Zobacz plik `LICENSE` w repozytorium.

## Autorzy

ZespÃ³Å‚ WronAI



### ğŸ“‹ **PrzeglÄ…d SkryptÃ³w**

1. **ğŸ” `quick_start_wronai.py`** - Zbieranie danych
   - Pobiera polskÄ… WikipediÄ™, Wolne Lektury
   - Fallback na syntetyczne dane
   - DomyÅ›lnie 500MB danych

2. **âš™ï¸ `processing.py`** - Przetwarzanie danych  
   - Czyszczenie i filtrowanie tekstÃ³w
   - Tokenizacja i chunking
   - Deduplikacja i train/val/test split

3. **ğŸ‹ï¸ `training.py`** - Trening modelu
   - QLoRA + 4-bit quantization
   - Optimized dla GPU/CPU
   - Tensorboard logging

4. **ğŸ¤– `inference.py`** - Inferowanie i ewaluacja
   - Generowanie tekstÃ³w
   - Gradio interface  
   - Performance benchmarking

5. **ğŸ¯ `pipeline.py`** - Master orchestrator
   - ZarzÄ…dza caÅ‚ym pipeline'm
   - CLI interface
   - State management

---

## ğŸš€ **Jak uruchomiÄ‡ (3 kroki):**

### **Krok 1: Setup**
```bash
python pipeline.py setup
pip install -r requirements.txt
```

### **Krok 2: PeÅ‚ny Pipeline**
```bash
python pipeline.py full --size 1000 --model microsoft/DialoGPT-medium
```

### **Krok 3: Uruchom model**
```bash
python scripts/pipeline.py infer
```

---

## ğŸ”§ **DostÄ™pne komendy:**

```bash
# SprawdÅº status
python scripts/pipeline.py status

# Tylko zbieranie danych (2GB)
python scripts/pipeline.py collect --size 2000

# Tylko trening (jeÅ›li dane gotowe)
python scripts/pipeline.py train --model "microsoft/DialoGPT-medium"

# WyczyÅ›Ä‡ workspace
python scripts/pipeline.py clean
```

---

## ğŸ“Š **Funkcje systemu:**

### âœ… **Smart Data Collection**
- Automatyczne fallback na dostÄ™pne ÅºrÃ³dÅ‚a
- Graceful error handling
- Progress tracking

### âœ… **Optimized Training**
- QLoRA dla efektywnoÅ›ci pamiÄ™ci
- 4-bit quantization
- Auto GPU/CPU detection
- Early stopping

### âœ… **Interactive Interface**
- Gradio web UI
- Chat interface
- Real-time generation

### âœ… **Interactive Interface**
- Gradio web UI
- Chat interface  
- Real-time generation
- Parameter tuning

### âœ… **Comprehensive Evaluation**
- Polish language capabilities testing
- Performance benchmarking
- Quality metrics
- Generation samples

### âœ… **Production Ready**
- State management
- Error recovery
- Logging & monitoring
- Modular architecture

---

## ğŸ¯ **Pipeline Flow:**

```
ğŸ“¥ Data Collection â†’ âš™ï¸ Processing â†’ ğŸ‹ï¸ Training â†’ ğŸ¤– Inference
     â†“                    â†“              â†“           â†“
  500MB+ Polish       Tokenized      WronAI      Gradio UI
  Text Corpus        Chunks         Model       + Chat
```

---

## ğŸ“ˆ **Expected Results:**

Po zakoÅ„czeniu pipeline'u otrzymasz:

### ğŸ“‚ **Workspace Structure:**
```
wronai_workspace/
â”œâ”€â”€ data/              # Surowe dane (Wikipedia, lektury)
â”œâ”€â”€ processed/         # Przetworzone chunki + tokeny
â”œâ”€â”€ model/            # Wytrenowany model WronAI
â”œâ”€â”€ logs/             # Logi treningu
â””â”€â”€ final_pipeline_report.json
```

### ğŸ¤– **Trained Model:**
- **Bazowy**: DialoGPT-medium (350M parametrÃ³w)
- **LoRA adapters**: ~16M trenowalnych parametrÃ³w  
- **JÄ™zyk**: Dostosowany do polskiego
- **Rozmiar**: ~700MB (z quantization)

### ğŸ“Š **Performance Metrics:**
- **Perplexity**: ~15-25 (im niÅ¼szy tym lepiej)
- **Generation speed**: 10-50 tokenÃ³w/s (GPU dependent)
- **Polish capability**: 70-85% success rate na testach

---

## ğŸ› ï¸ **Troubleshooting:**

### **Problem: Brak GPU**
```bash
# Pipeline automatycznie przeÅ‚Ä…czy na CPU
# Trening bÄ™dzie wolniejszy ale zadziaÅ‚a
python scripts/pipeline.py full --size 100  # Mniejszy dataset
```

### **Problem: MaÅ‚o pamiÄ™ci**
```bash
# Zmniejsz batch size w wronai_training.py
batch_size = 1  # zamiast 4
gradient_accumulation_steps = 32  # zamiast 8
```

### **Problem: Dane siÄ™ nie pobierajÄ…**
```bash
# SprawdÅº czy masz internet i uruchom tylko data collection
python scripts/pipeline.py collect --size 500
```

### **Problem: Model nie generuje dobrze**
```bash
# Dostraj parametry w interfejsie:
# - Temperature: 0.7-0.9 (kreatywnoÅ›Ä‡)
# - Top-p: 0.8-0.95 (rÃ³Å¼norodnoÅ›Ä‡)
# - Max length: 100-300 (dÅ‚ugoÅ›Ä‡)
```

---

## ğŸ”® **Dalszy rozwÃ³j:**

### **Immediate improvements:**
1. **WiÄ™cej danych**: ZwiÄ™ksz `--size` do 5000+ MB
2. **Lepszy model bazowy**: UÅ¼yj `mistralai/Mistral-7B-v0.1`
3. **Domain adaptation**: Dodaj specjalistyczne korpusy

### **Advanced features:**
1. **RLHF**: Reinforcement Learning from Human Feedback
2. **Multimodal**: Dodaj obsÅ‚ugÄ™ obrazÃ³w
3. **RAG**: Retrieval Augmented Generation
4. **Fine-tuning**: Task-specific adapters

### **Production deployment:**
1. **API server**: FastAPI + uvicorn
2. **Docker**: Containerization
3. **Monitoring**: MLflow + Prometheus
4. **Scaling**: Kubernetes deployment

---

## ğŸ“š **NastÄ™pne kroki:**

1. **Uruchom pipeline** - `python scripts/pipeline.py full`
2. **Przetestuj model** w interfejsie Gradio
3. **OceÅ„ wyniki** przez generation samples
4. **Iteruj** - dostrajaj parametry i zwiÄ™kszaj dane
5. **Deploy** - stwÃ³rz API dla aplikacji

---

## ğŸ† **Sukces oznacza:**

âœ… **Model generuje sensowne polskie teksty**  
âœ… **Rozumie kontekst i gramatykÄ™**  
âœ… **Interface dziaÅ‚a pÅ‚ynnie**  
âœ… **Performance jest akceptowalny**  
âœ… **System jest skalowalny i rozszerzalny**



