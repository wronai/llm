# WronAI Data Collection Pipeline

System do pobierania i przetwarzania polskich danych treningowych dla modeli jÄ™zykowych.

## Opis projektu

WronAI Data Collection Pipeline to kompleksowe narzÄ™dzie do zbierania, czyszczenia i przetwarzania polskojÄ™zycznych danych tekstowych z rÃ³Å¼nych ÅºrÃ³deÅ‚. System zostaÅ‚ zaprojektowany do tworzenia wysokiej jakoÅ›ci korpusu treningowego dla modeli jÄ™zykowych w jÄ™zyku polskim.

## Å¹rÃ³dÅ‚a danych

Pipeline zbiera dane z nastÄ™pujÄ…cych ÅºrÃ³deÅ‚:

1. **Wysokiej jakoÅ›ci**:
   - Polska Wikipedia
   - Wolne Lektury (polskie ksiÄ…Å¼ki w domenie publicznej)
   - ArtykuÅ‚y akademickie

2. **Åšredniej jakoÅ›ci**:
   - OSCAR Polish (filtrowany Common Crawl)
   - Common Crawl (wybrane polskie domeny)

## FunkcjonalnoÅ›ci

- Pobieranie danych z wielu ÅºrÃ³deÅ‚
- Czyszczenie i normalizacja tekstu
- Deduplikacja na poziomie dokumentÃ³w
- Identyfikacja jÄ™zyka (filtrowanie niepolskich tekstÃ³w)
- Tworzenie podziaÅ‚Ã³w na zbiory treningowe, walidacyjne i testowe
- Generowanie metadanych

## Wymagania systemowe

- Python 3.8 lub nowszy
- DostÄ™p do internetu
- Min. 16GB RAM (zalecane)
- PrzestrzeÅ„ dyskowa: min. 100GB (zaleÅ¼nie od docelowego rozmiaru korpusu)

## Instalacja

```bash
# Sklonuj repozytorium
git clone https://github.com/wronai/llm.git
cd llm/wronai_data

# Uruchom skrypt instalacyjny
bash setup.sh
```

Skrypt `setup.sh` automatycznie:
1. Tworzy wirtualne Å›rodowisko Python
2. Instaluje wszystkie wymagane zaleÅ¼noÅ›ci z pliku requirements.txt
3. Pobiera model FastText do identyfikacji jÄ™zyka

## Autorzy

ZespÃ³Å‚ WronAI

## ğŸ“‹ **PrzeglÄ…d SkryptÃ³w WronAI**

1. **ğŸ” `collect_wronai_data.py`** - GÅ‚Ã³wny skrypt zbierania danych
   - Pobiera polskÄ… WikipediÄ™, OSCAR, Wolne Lektury i inne ÅºrÃ³dÅ‚a
   - ObsÅ‚uguje rÃ³Å¼ne formaty i ÅºrÃ³dÅ‚a danych
   - PeÅ‚na implementacja pipeline'u zbierania danych

2. **ğŸ” `collect_wronai_data_fixed.py`** - Naprawiona wersja skryptu zbierania danych
   - UÅ¼ywa dostÄ™pnych ÅºrÃ³deÅ‚ danych bez wymagania specjalnego dostÄ™pu
   - Zawiera mechanizmy fallback dla niedostÄ™pnych ÅºrÃ³deÅ‚
   - Lepsze zarzÄ…dzanie bÅ‚Ä™dami i obsÅ‚uga wyjÄ…tkÃ³w

3. **ğŸ” `quick_data_collection.py`** - Uproszczona wersja zbierania danych
   - Szybkie uruchomienie bez problemÃ³w z dostÄ™pem do danych
   - Minimalna wersja bez zewnÄ™trznych zaleÅ¼noÅ›ci
   - DomyÅ›lnie zbiera 500MB danych

4. **âš™ï¸ `processor.py`** - Przetwarzanie danych
   - Czyszczenie i filtrowanie tekstÃ³w
   - Tokenizacja i chunking
   - Deduplikacja i train/val/test split
   - Przygotowanie danych do formatu treningowego

5. **ğŸ‹ï¸ `trainer.py`** - Trening modelu
   - QLoRA + 4-bit quantization
   - Optimized dla GPU/CPU
   - Early stopping i monitoring treningu
   - ObsÅ‚uga rÃ³Å¼nych modeli bazowych

6. **ğŸ¤– `inference.py`** - Inferowanie i ewaluacja
   - Generowanie tekstÃ³w z wytrenowanego modelu
   - Gradio interface dla interaktywnego testowania
   - Ewaluacja jakoÅ›ci modelu
   - Custom stopping criteria dla polskiego tekstu

7. **ğŸ¯ `pipeline.py`** - Master orchestrator
   - ZarzÄ…dza caÅ‚ym pipeline'm od zbierania danych do inferowania
   - CLI interface z rÃ³Å¼nymi komendami
   - State management i Å›ledzenie postÄ™pu
   - Automatyczne tworzenie wymaganych katalogÃ³w i plikÃ³w

8. **ğŸ§ª `test_data_collection.py`** - Testy zbierania danych
   - Szybkie testy funkcjonalnoÅ›ci zbierania danych
   - Mniejszy zestaw danych (100MB)
   - Weryfikacja poprawnoÅ›ci dziaÅ‚ania pipeline'u

## ğŸ“‹ **SzczegÃ³Å‚owy opis skryptÃ³w**

### 1. **collect_wronai_data.py**
GÅ‚Ã³wny skrypt zbierania danych dla WronAI. Implementuje peÅ‚ny pipeline pobierania danych z rÃ³Å¼nych ÅºrÃ³deÅ‚ polskich tekstÃ³w.

```python
# GÅ‚Ã³wne funkcje:
- WronAIDataCollector - klasa zarzÄ…dzajÄ…ca caÅ‚ym procesem zbierania
- collect_wikipedia_polish() - pobiera artykuÅ‚y z polskiej Wikipedii
- collect_oscar_polish() - pobiera teksty z korpusu OSCAR
- collect_wolne_lektury() - pobiera ksiÄ…Å¼ki z Wolnych Lektur
- collect_github_corpora() - pobiera korpusy z GitHub
- run_collection_pipeline() - uruchamia caÅ‚y proces zbierania danych
```

### 2. **collect_wronai_data_fixed.py**
Naprawiona wersja skryptu zbierania danych, ktÃ³ra rozwiÄ…zuje problemy z dostÄ™pem do ÅºrÃ³deÅ‚ i zawiera lepsze mechanizmy obsÅ‚ugi bÅ‚Ä™dÃ³w.

```python
# GÅ‚Ã³wne funkcje:
- WronAICollector - uproszczona klasa do zbierania danych
- collect_wikipedia_polish() - z mechanizmem fallback
- collect_wikipedia_fallback() - alternatywna metoda pobierania z mC4
- collect_wolne_lektury() - z lepszym error handling
- clean_text() - czyszczenie i normalizacja tekstu
- run() - uruchamia caÅ‚y proces zbierania danych
```

### 3. **quick_data_collection.py**
Uproszczona wersja skryptu zbierania danych, dziaÅ‚ajÄ…ca z minimalnymi zaleÅ¼noÅ›ciami i zaprojektowana do szybkiego uruchomienia.

```python
# GÅ‚Ã³wne funkcje:
- SimpleWronAICollector - minimalistyczna klasa do zbierania danych
- collect_wikipedia_simple() - uproszczone pobieranie z Wikipedii
- collect_wolne_lektury_simple() - uproszczone pobieranie z Wolnych Lektur
- generate_synthetic_data() - generowanie syntetycznych danych jako fallback
- is_polish_simple() - prosta detekcja jÄ™zyka polskiego
- run_quick_collection() - szybki proces zbierania danych
```

### 4. **processor.py**
Skrypt do przetwarzania zebranych danych tekstowych, przygotowujÄ…cy je do treningu modelu.

```python
# GÅ‚Ã³wne funkcje:
- WronAIDataProcessor - klasa do przetwarzania danych
- load_raw_data() - Å‚adowanie surowych danych z plikÃ³w JSONL
- clean_and_filter() - czyszczenie i filtrowanie tekstÃ³w
- tokenize_and_chunk() - tokenizacja i dzielenie na chunki
- deduplicate() - usuwanie duplikatÃ³w
- create_train_val_test_split() - podziaÅ‚ na zbiory treningowe/walidacyjne/testowe
- save_processed_data() - zapisywanie przetworzonych danych
- run_processing_pipeline() - uruchamia caÅ‚y proces przetwarzania
```

### 5. **trainer.py**
Skrypt do treningu modelu jÄ™zykowego z wykorzystaniem technik QLoRA i 4-bit quantization.

```python
# GÅ‚Ã³wne funkcje:
- WronAITrainer - klasa do treningu modelu
- load_datasets() - Å‚adowanie przetworzonych datasetÃ³w
- setup_model() - konfiguracja modelu z kwantyzacjÄ… i LoRA
- train_model() - trening modelu
- evaluate_model() - ewaluacja wytrenowanego modelu
- generate_sample_text() - generowanie przykÅ‚adowego tekstu
- run_full_training_pipeline() - uruchamia caÅ‚y proces treningu
```

### 6. **inference.py**
Skrypt do inferowania z wytrenowanego modelu, zawierajÄ…cy interfejs Gradio do interaktywnego testowania.

```python
# GÅ‚Ã³wne funkcje:
- WronAIInference - klasa do inferowania z modelu
- PolishStoppingCriteria - niestandardowe kryteria zatrzymania dla polskiego tekstu
- generate_text() - generowanie tekstu na podstawie promptu
- chat() - konwersacja z modelem
- evaluate_polish_capabilities() - ewaluacja zdolnoÅ›ci modelu w jÄ™zyku polskim
- benchmark_performance() - benchmark wydajnoÅ›ci modelu
```

### 7. **pipeline.py**
Master orchestrator zarzÄ…dzajÄ…cy caÅ‚ym procesem od zbierania danych do inferowania.

```python
# GÅ‚Ã³wne funkcje:
- WronAIMasterPipeline - klasa zarzÄ…dzajÄ…ca caÅ‚ym pipeline'm
- run_data_collection() - uruchamia zbieranie danych
- run_data_processing() - uruchamia przetwarzanie danych
- run_model_training() - uruchamia trening modelu
- run_inference() - uruchamia inferowanie
- run_full_pipeline() - uruchamia caÅ‚y proces od poczÄ…tku do koÅ„ca
- create_requirements_file() - tworzy plik requirements.txt
```

### 8. **test_data_collection.py**
Skrypt do testowania funkcjonalnoÅ›ci zbierania danych na maÅ‚ym zestawie danych.

```python
# GÅ‚Ã³wne funkcje:
- test_small_collection() - test zbierania danych z maÅ‚ym rozmiarem (100MB)
```

## ğŸš€ **Jak uruchomiÄ‡ poszczegÃ³lne skrypty:**

### Zbieranie danych:
```bash
# PeÅ‚ny pipeline zbierania danych
python3 scripts/collect_wronai_data.py

# Naprawiona wersja z lepszÄ… obsÅ‚ugÄ… bÅ‚Ä™dÃ³w
python3 scripts/collect_wronai_data_fixed.py

# Szybka wersja z minimalnymi zaleÅ¼noÅ›ciami
python3 scripts/quick_data_collection.py

# Test zbierania danych (100MB)
python3 scripts/test_data_collection.py
```

### Przetwarzanie danych:
```bash
# Przetwarzanie zebranych danych
python3 scripts/processor.py --input_dir ./data --output_dir ./processed
```

### Trening modelu:
```bash
# Trening modelu z domyÅ›lnymi parametrami
python3 scripts/trainer.py

# Trening z niestandardowymi parametrami
python3 scripts/trainer.py --model_name "microsoft/DialoGPT-medium" --data_dir ./processed --output_dir ./model
```

### Inferowanie:
```bash
# Uruchomienie interfejsu Gradio
python3 scripts/inference.py --model_path ./model
```

### PeÅ‚ny pipeline:
```bash
# Uruchomienie caÅ‚ego pipeline'u
python3 scripts/pipeline.py full --size 1000 --model "microsoft/DialoGPT-medium"

# Tylko zbieranie danych
python3 scripts/pipeline.py collect --size 2000

# Tylko trening
python3 scripts/pipeline.py train --model "microsoft/DialoGPT-medium"

# Tylko inferowanie
python3 scripts/pipeline.py infer
```

## ğŸ”§ **DostÄ™pne komendy pipeline:**

```bash
# SprawdÅº status pipeline'u
python3 scripts/pipeline.py status

# Tylko zbieranie danych (2GB)
python3 scripts/pipeline.py collect --size 2000

# Tylko trening (jeÅ›li dane gotowe)
python3 scripts/pipeline.py train --model "microsoft/DialoGPT-medium"

# PeÅ‚ny pipeline
python3 scripts/pipeline.py full --size 1000 --model "microsoft/DialoGPT-medium"

# Uruchom inferowanie
python3 scripts/pipeline.py infer

# WyczyÅ›Ä‡ workspace
python3 scripts/pipeline.py clean
```

## ğŸ“Š **Funkcje systemu:**

### âœ… **Smart Data Collection**
- Automatyczne fallback na dostÄ™pne ÅºrÃ³dÅ‚a
- Graceful error handling
- Progress tracking
- ObsÅ‚uga wielu formatÃ³w i ÅºrÃ³deÅ‚ danych

### âœ… **Optimized Training**
- QLoRA dla efektywnoÅ›ci pamiÄ™ci
- 4-bit quantization
- Auto GPU/CPU detection
- Early stopping

### âœ… **Interactive Interface**
- Gradio web UI
- Chat interface
- Real-time generation
- Parameter tuning

### âœ… **Comprehensive Evaluation**
- Polish language capabilities testing
- Performance benchmarking
- Quality metrics
- Generation samples

### âœ… **Production Ready**
- State management
- Error recovery
- Logging & monitoring
- Modular architecture

## ğŸ¯ **Pipeline Flow:**

```
ğŸ“¥ Data Collection â†’ âš™ï¸ Processing â†’ ğŸ‹ï¸ Training â†’ ğŸ¤– Inference
     â†“                    â†“              â†“           â†“
  500MB+ Polish       Tokenized      WronAI      Gradio UI
  Text Corpus        Chunks         Model       + Chat
```

## ğŸ“ˆ **Expected Results:**

Po zakoÅ„czeniu pipeline'u otrzymasz:

### ğŸ“‚ **Workspace Structure:**
```
wronai_workspace/
â”œâ”€â”€ data/              # Surowe dane (Wikipedia, lektury)
â”œâ”€â”€ processed/         # Przetworzone chunki + tokeny
â”œâ”€â”€ model/            # Wytrenowany model WronAI
â”œâ”€â”€ logs/             # Logi treningu
â””â”€â”€ pipeline_state.json
```

### ğŸ¤– **Trained Model:**
- **Bazowy**: DialoGPT-medium (350M parametrÃ³w)
- **LoRA adapters**: ~16M trenowalnych parametrÃ³w  
- **JÄ™zyk**: Dostosowany do polskiego
- **Rozmiar**: ~700MB (z quantization)

## ğŸ› ï¸ **Troubleshooting:**

### **Problem: Brak dostÄ™pu do danych Wikipedia**
```bash
# Skrypt automatycznie uÅ¼yje alternatywnych ÅºrÃ³deÅ‚
python3 run_scripts.py fixed
```

### **Problem: Brak GPU**
```bash
# Pipeline automatycznie przeÅ‚Ä…czy na CPU
# Trening bÄ™dzie wolniejszy ale zadziaÅ‚a
python3 scripts/pipeline.py full --size 100  # Mniejszy dataset
```

### **Problem: MaÅ‚o pamiÄ™ci**
```bash
# Zmniejsz batch size w scripts/trainer.py
# Edytuj parametry:
batch_size = 1  # zamiast 4
gradient_accumulation_steps = 32  # zamiast 8
```

### **Problem: Dane siÄ™ nie pobierajÄ…**
```bash
# SprawdÅº czy masz internet i uruchom tylko data collection
python3 scripts/pipeline.py collect --size 500
```

## ğŸ”® **Dalszy rozwÃ³j:**

### **Immediate improvements:**
1. **WiÄ™cej danych**: ZwiÄ™ksz `--size` do 5000+ MB
2. **Lepszy model bazowy**: UÅ¼yj `mistralai/Mistral-7B-v0.1`
3. **Domain adaptation**: Dodaj specjalistyczne korpusy

### **Advanced features:**
1. **RLHF**: Reinforcement Learning from Human Feedback
2. **Multimodal**: Dodaj obsÅ‚ugÄ™ obrazÃ³w
3. **RAG**: Retrieval Augmented Generation
4. **Fine-tuning**: Task-specific adapters

## ğŸ“š **NastÄ™pne kroki:**

1. **Uruchom pipeline** - `python3 scripts/pipeline.py full`
2. **Przetestuj model** w interfejsie Gradio
3. **OceÅ„ wyniki** przez generation samples
4. **Iteruj** - dostrajaj parametry i zwiÄ™kszaj dane
5. **Deploy** - stwÃ³rz API dla aplikacji
