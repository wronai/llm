# WronAI ğŸ¦â€â¬›

**Polski model jÄ™zykowy inspirowany PLLuM - demokratyzacja AI dla jÄ™zyka polskiego**

WronAI to open-source projekt majÄ…cy na celu stworzenie efektywnego polskiego modelu jÄ™zykowego, ktÃ³ry
moÅ¼na trenowaÄ‡ i uruchamiaÄ‡ na sprzÄ™cie konsumenckim. 
Projekt wykorzystuje najnowsze techniki optymalizacji jak QLoRA, gradient checkpointing i kwantyzacjÄ™ 
do osiÄ…gniÄ™cia maksymalnej wydajnoÅ›ci przy minimalnych wymaganiach sprzÄ™towych.

## ğŸ¯ Cele projektu

- **DostÄ™pnoÅ›Ä‡**: Model moÅ¼liwy do uruchomienia na GPU 8GB+
- **JÄ™zyk polski**: Specjalizacja w przetwarzaniu jÄ™zyka polskiego
- **Open Source**: PeÅ‚na otwartoÅ›Ä‡ kodu i danych treningowych
- **SpoÅ‚ecznoÅ›Ä‡**: Budowa ekosystemu wokÃ³Å‚ polskich modeli AI

## ğŸ—ï¸ Architektura

WronAI bazuje na sprawdzonych rozwiÄ…zaniach:
- **Model bazowy**: Mistral-7B z QLoRA fine-tuningiem
- **Kwantyzacja**: 4-bitowa NF4 dla optymalizacji pamiÄ™ci
- **Corpus**: ~50GB polskich tekstÃ³w wysokiej jakoÅ›ci
- **Alignment**: Polski dataset preferencji dla RLHF

## ğŸš€ Szybki start

```bash
# Klonowanie repozytorium
git clone https://github.com/wronai/llm.git
cd llm

# Utworzenie i aktywacja wirtualnego Å›rodowiska (zalecane)
python -m venv wronai-env
source wronai-env/bin/activate  # Linux/Mac
# wronai-env\Scripts\activate  # Windows

# Instalacja dependencies
pip install -r requirements.txt

# Alternatywna instalacja w przypadku problemÃ³w (instalacja pakietÃ³w pojedynczo)
# pip install torch transformers accelerate peft datasets evaluate
# pip install bitsandbytes scipy tokenizers sentencepiece regex spacy
# pip install beautifulsoup4 requests aiohttp scrapy
# pip install pyyaml omegaconf loguru rich
# pip install wandb

# Przygotowanie danych
python scripts/prepare_data.py

# Trening modelu
python scripts/train.py --config configs/default.yaml

# Inferencja
python scripts/inference.py --model checkpoints/wronai-7b --prompt "Opowiedz o Polsce"
```

> **Uwaga**: JeÅ›li napotkasz problem z instalacjÄ… modelu jÄ™zyka polskiego (`pl_core_news_sm`), moÅ¼esz kontynuowaÄ‡ pracÄ™ z projektem. Model ten jest opcjonalny i uÅ¼ywany tylko do niektÃ³rych zaawansowanych funkcji przetwarzania tekstu.

## ğŸ“Š Wyniki

| Model | Parametry | VRAM | Polish Score | Licensing |
|-------|-----------|------|--------------|-----------|
| WronAI-7B | 7B | 8GB | 7.2/10 | Apache 2.0 |
| PLLuM-8x7B | 46.7B | 40GB+ | 8.5/10 | Custom |
| Bielik-7B | 7B | 14GB | 7.8/10 | Apache 2.0 |

## ğŸ› ï¸ Wymagania systemowe

### Minimalne (trening)
- **GPU**: NVIDIA RTX 3070/4060 Ti (8GB VRAM)
- **RAM**: 16GB DDR4
- **Storage**: 100GB wolnego miejsca
- **OS**: Linux/Windows + CUDA 11.8+

### Zalecane (trening)
- **GPU**: NVIDIA RTX 4080/4090 (16GB+ VRAM)
- **RAM**: 32GB DDR4/DDR5
- **Storage**: 500GB NVMe SSD
- **OS**: Ubuntu 22.04 LTS

### Inferencja
- **GPU**: 6GB VRAM (z kwantyzacjÄ…)
- **RAM**: 8GB
- **Storage**: 4GB dla modelu

## ğŸ“š Dokumentacja

- [Instalacja](docs/installation.md)
- [Trening modelu](docs/training.md)
- [Wykorzystanie modelu](docs/inference.md)
- [API Reference](docs/api.md)
- [Benchmark](docs/benchmarks.md)
- [FAQ](docs/faq.md)

## ğŸ—‚ï¸ Struktura projektu

```
WronAI/
â”œâ”€â”€ configs/          # Konfiguracje treningowe
â”œâ”€â”€ data/            # Skrypty do obsÅ‚ugi danych
â”œâ”€â”€ docs/            # Dokumentacja
â”œâ”€â”€ models/          # Definicje architektur
â”œâ”€â”€ scripts/         # Skrypty treningowe i inferencji
â”œâ”€â”€ tests/           # Testy jednostkowe
â”œâ”€â”€ notebooks/       # Jupyter notebooks z przykÅ‚adami
â”œâ”€â”€ checkpoints/     # Wytrenowane modele
â””â”€â”€ requirements.txt # ZaleÅ¼noÅ›ci Python
```

## ğŸ¤ WkÅ‚ad w projekt

Zapraszamy do wspÃ³Å‚pracy! Zobacz [CONTRIBUTING.md](CONTRIBUTING.md) aby dowiedzieÄ‡ siÄ™ jak moÅ¼esz pomÃ³c:

- ğŸ› ZgÅ‚aszanie bÅ‚Ä™dÃ³w
- ğŸ’¡ Propozycje nowych funkcji
- ğŸ“ Poprawa dokumentacji
- ğŸ”§ Implementacja nowych features
- ğŸ“Š Dodawanie benchmarkÃ³w

## ğŸ† OsiÄ…gniÄ™cia

- âœ… Model trenowany na <8GB VRAM
- âœ… Polski corpus 50GB+ wysokiej jakoÅ›ci
- âœ… RLHF alignment dla jÄ™zyka polskiego
- âœ… Integracja z Hugging Face Hub
- âœ… Docker containers dla Å‚atwego wdroÅ¼enia
- ğŸ”„ Web interface (w trakcie)
- ğŸ”„ Mobile app (planowane)

## ğŸ“ˆ Roadmap

### v0.1 (Aktualna)
- [x] Podstawowy QLoRA fine-tuning
- [x] Polski corpus przygotowanie
- [x] Baseline benchmarki

### v0.2 (Q2 2025)
- [ ] RLHF implementation
- [ ] Multi-GPU training support
- [ ] Web interface
- [ ] API endpoints

### v0.3 (Q3 2025)
- [ ] Mixture of Experts (MoE)
- [ ] Retrieval Augmented Generation (RAG)
- [ ] Mobile deployment
- [ ] Enterprise features

### v1.0 (Q4 2025)
- [ ] Production-ready release
- [ ] Full documentation
- [ ] Commercial support
- [ ] Community ecosystem

## ğŸ–ï¸ ZespÃ³Å‚

- **GÅ‚Ã³wny developer**: [@tom-sapletta-com](https://github.com/tom-sapletta-com)
- **Lingwista komputacyjny**: Potrzebny volunteer
- **DevOps**: Potrzebny volunteer
- **Community manager**: Potrzebny volunteer

## ğŸ“„ Licencja

Ten projekt jest dostÄ™pny na licencji Apache 2.0. Zobacz [LICENSE](LICENSE) po szczegÃ³Å‚y.

### Licencje danych

- **Otwarte dane**: Apache 2.0, CC-BY-SA (komercyjne OK)
- **Dane badawcze**: Tylko do celÃ³w niekomercyjnych
- **Model weights**: Apache 2.0

## ğŸ™ PodziÄ™kowania

- **Bielik Team** za inspiracjÄ™ i wsparcie 
- **Mistral AI** za model bazowy
- **Hugging Face** za infrastrukturÄ™
- **Polish NLP Community** za wsparcie
- **CLARIN-PL** za resources

## ğŸ“ Kontakt

- **Issues**: [GitHub Issues](https://github.com/wronai/llm/issues)
- **Discussions**: [GitHub Discussions](https://github.com/wronai/llm/discussions)
- **Email**: info@softreck.dev
- **Discord**: [WronAI Community](https://discord.gg/wronai)

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=wronai/llm&type=Date)](https://star-history.com/#wronai/llm&Date)

---

**WronAI** - Demokratyzacja polskiej sztucznej inteligencji ğŸ‡µğŸ‡±ğŸ¤–